In the longer traces, the instruction cache missed much less than the data cache. Mostly because instructions are being repeated in loops, as well as functions. Larger cache sizes also vastly decreased the miss rate in the instruction cache by several place values. This trend was also noticeable in the data cache but to a much lower degree. Total cycle time also decreased when the size of the cache increased. This is because the miss rate decreased, while the miss penalty stayed the same. This would not be the case in a real cache, where it would take time to search through a large cache. 
	Associativity seems to have a mostly negative effect on both the miss rate and the total number of cycles. 
A larger block size decreased the miss rate and total cycle count for the clear majority of the experiments. The exception is that total execution cycles increased with block size with the direct mapped caches on sample large 2. Pulling more data into the cache at once definitely saved time by causing us to miss and have to reach out to secondary memory less often.
